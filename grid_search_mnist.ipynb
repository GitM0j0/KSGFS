{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "# from itertools import chain\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "dataset_size = 55000\n",
    "validation_size = 5000\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load MNIST training set\n",
    "train_data = torchvision.datasets.MNIST(root=os.environ.get(\"DATASETS_PATH\", \"~/datasets\"), train=True,\n",
    "                                         download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split training set into training and validation set\n",
    "train_data, validation_data = torch.utils.data.random_split(train_data,[dataset_size, validation_size])\n",
    "# Load MNIST test set\n",
    "test_data = torchvision.datasets.MNIST(root=os.environ.get(\"DATASETS_PATH\", \"~/datasets\"), train=False,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Put datasets into data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(validation_data, batch_size=validation_size, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                      nn.Linear(784, 400, bias=True),\n",
    "                      nn.SELU(),\n",
    "                      nn.Linear(400, 400, bias=True),\n",
    "                      nn.SELU(),\n",
    "                      nn.Linear(400, 400, bias=True),\n",
    "                      nn.SELU(),\n",
    "                      nn.Linear(400,10, bias=True)\n",
    "                    ).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluation(object):\n",
    "    def __init__(self, test_data, criterion):\n",
    "        self.test_data = test_data\n",
    "        self.criterion = criterion\n",
    "        self.n = 0\n",
    "        self.avg_prediction = 0.\n",
    "\n",
    "    def acc(self, model):\n",
    "\n",
    "        with torch.autograd.no_grad():\n",
    "            self.n += 1\n",
    "            for x, y in iter(self.test_data):\n",
    "                x = x.view(x.size(0), -1)\n",
    "                predictions = model(x)\n",
    "                self.avg_prediction = self.avg_prediction * (self.n - 1.) / self.n + predictions / self.n\n",
    "                \n",
    "\n",
    "                loss = self.criterion(self.avg_prediction, y)\n",
    "                acc = 100 * (self.avg_prediction.argmax(1) == y).float().sum() / x.shape[0]\n",
    "\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 57.2055 - acc: 0.9500\n",
      "Epoch 1 - loss: 30.3829 - acc: 0.9700\n",
      "Epoch 2 - loss: 27.6055 - acc: 0.9700\n",
      "Epoch 3 - loss: 25.3094 - acc: 0.9700\n",
      "Epoch 4 - loss: 22.7366 - acc: 0.9800\n",
      "Epoch 5 - loss: 19.7662 - acc: 0.9700\n",
      "Epoch 6 - loss: 16.9395 - acc: 0.9900\n",
      "Epoch 7 - loss: 14.6410 - acc: 0.9900\n",
      "Epoch 8 - loss: 12.9335 - acc: 0.9900\n",
      "Epoch 9 - loss: 11.5576 - acc: 0.9900\n",
      "Epoch 10 - loss: 10.5312 - acc: 0.9900\n",
      "Epoch 11 - loss: 9.5251 - acc: 0.9900\n",
      "Epoch 12 - loss: 8.7791 - acc: 0.9900\n",
      "Epoch 13 - loss: 8.0002 - acc: 0.9900\n",
      "Epoch 14 - loss: 7.2921 - acc: 0.9900\n",
      "Epoch 15 - loss: 6.7395 - acc: 1.0000\n",
      "Epoch 16 - loss: 6.2893 - acc: 1.0000\n",
      "Epoch 17 - loss: 5.9102 - acc: 0.9800\n",
      "Epoch 18 - loss: 4.8733 - acc: 1.0000\n",
      "Epoch 19 - loss: 4.3376 - acc: 0.9800\n",
      "Epoch 20 - loss: 4.1184 - acc: 0.9900\n",
      "Epoch 21 - loss: 3.9371 - acc: 1.0000\n",
      "Epoch 22 - loss: 3.7535 - acc: 1.0000\n",
      "Epoch 23 - loss: 3.6029 - acc: 1.0000\n",
      "Epoch 24 - loss: 3.4409 - acc: 1.0000\n",
      "Epoch 25 - loss: 3.3971 - acc: 0.9900\n",
      "Epoch 26 - loss: 3.1925 - acc: 0.9900\n",
      "Epoch 27 - loss: 3.1157 - acc: 1.0000\n",
      "Epoch 28 - loss: 3.0403 - acc: 1.0000\n",
      "Epoch 29 - loss: 2.9361 - acc: 0.9900\n",
      "Epoch 30 - loss: 2.8133 - acc: 0.9900\n",
      "Epoch 31 - loss: 2.7117 - acc: 1.0000\n",
      "Epoch 32 - loss: 2.6348 - acc: 0.9900\n",
      "Epoch 33 - loss: 2.5956 - acc: 0.9900\n",
      "Epoch 34 - loss: 2.5511 - acc: 0.9900\n",
      "Epoch 35 - loss: 2.5483 - acc: 1.0000\n",
      "Epoch 36 - loss: 2.1869 - acc: 0.9900\n",
      "Epoch 37 - loss: 1.9116 - acc: 1.0000\n",
      "Epoch 38 - loss: 1.9370 - acc: 1.0000\n",
      "Epoch 39 - loss: 1.9255 - acc: 1.0000\n",
      "Epoch 40 - loss: 1.9347 - acc: 1.0000\n",
      "Epoch 41 - loss: 1.8459 - acc: 1.0000\n",
      "Epoch 42 - loss: 1.8272 - acc: 1.0000\n",
      "Epoch 43 - loss: 1.7540 - acc: 1.0000\n",
      "Epoch 44 - loss: 1.8319 - acc: 0.9800\n",
      "Epoch 45 - loss: 1.7616 - acc: 0.9700\n",
      "Epoch 46 - loss: 1.8288 - acc: 0.9900\n",
      "Epoch 47 - loss: 1.8106 - acc: 0.9900\n",
      "Epoch 48 - loss: 1.7238 - acc: 1.0000\n",
      "Epoch 49 - loss: 1.7455 - acc: 0.9900\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: 47.8379 - acc: 0.9700\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: 43.5362 - acc: 0.9600\n",
      "Epoch 1 - loss: 25.5729 - acc: 0.9700\n",
      "Epoch 2 - loss: 19.0409 - acc: 0.9700\n",
      "Epoch 3 - loss: 16.2206 - acc: 0.9900\n",
      "Epoch 4 - loss: 15.3466 - acc: 1.0000\n",
      "Epoch 5 - loss: 14.3422 - acc: 1.0000\n",
      "Epoch 6 - loss: 14.3609 - acc: 0.9600\n",
      "Epoch 7 - loss: 15.1967 - acc: 0.9900\n",
      "Epoch 8 - loss: 13.7519 - acc: 0.9900\n",
      "Epoch 9 - loss: 14.6608 - acc: 0.9800\n",
      "Epoch 10 - loss: 14.7086 - acc: 0.9900\n",
      "Epoch 11 - loss: 14.5539 - acc: 0.9900\n",
      "Epoch 12 - loss: 16.3128 - acc: 0.9800\n",
      "Epoch 13 - loss: 14.8930 - acc: 1.0000\n",
      "Epoch 14 - loss: 15.3856 - acc: 0.9900\n",
      "Epoch 15 - loss: 15.6684 - acc: 0.9800\n",
      "Epoch 16 - loss: 16.7364 - acc: 0.9800\n",
      "Epoch 17 - loss: 14.8164 - acc: 0.9900\n",
      "Epoch 18 - loss: 8.0842 - acc: 1.0000\n",
      "Epoch 19 - loss: 3.3562 - acc: 1.0000\n",
      "Epoch 20 - loss: 2.2547 - acc: 1.0000\n",
      "Epoch 21 - loss: 2.6273 - acc: 1.0000\n",
      "Epoch 22 - loss: 2.7477 - acc: 0.9900\n",
      "Epoch 23 - loss: 3.0152 - acc: 1.0000\n",
      "Epoch 24 - loss: 3.8050 - acc: 1.0000\n",
      "Epoch 25 - loss: 3.2718 - acc: 1.0000\n",
      "Epoch 26 - loss: 3.4031 - acc: 1.0000\n",
      "Epoch 27 - loss: 3.9586 - acc: 0.9900\n",
      "Epoch 28 - loss: 4.8254 - acc: 1.0000\n",
      "Epoch 29 - loss: 4.6306 - acc: 0.9800\n",
      "Epoch 30 - loss: 5.6754 - acc: 0.9800\n",
      "Epoch 31 - loss: 5.3280 - acc: 1.0000\n",
      "Epoch 32 - loss: 4.2064 - acc: 1.0000\n",
      "Epoch 33 - loss: 6.2172 - acc: 1.0000\n",
      "Epoch 34 - loss: 6.2487 - acc: 1.0000\n",
      "Epoch 35 - loss: 5.3381 - acc: 1.0000\n",
      "Epoch 36 - loss: 4.2506 - acc: 1.0000\n",
      "Epoch 37 - loss: 1.1711 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.8139 - acc: 1.0000\n",
      "Epoch 39 - loss: 1.1471 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.9905 - acc: 1.0000\n",
      "Epoch 41 - loss: 1.3070 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.8426 - acc: 0.9900\n",
      "Epoch 43 - loss: 0.9943 - acc: 1.0000\n",
      "Epoch 44 - loss: 0.8434 - acc: 1.0000\n",
      "Epoch 45 - loss: 1.3689 - acc: 1.0000\n",
      "Epoch 46 - loss: 1.2823 - acc: 1.0000\n",
      "Epoch 47 - loss: 1.3135 - acc: 1.0000\n",
      "Epoch 48 - loss: 1.4258 - acc: 1.0000\n",
      "Epoch 49 - loss: 1.6263 - acc: 1.0000\n",
      "Epoch 0 - loss: 42.8485 - acc: 0.9600\n",
      "Epoch 1 - loss: 26.8914 - acc: 0.9700\n",
      "Epoch 2 - loss: 18.4915 - acc: 0.9700\n",
      "Epoch 3 - loss: 14.4481 - acc: 0.9800\n",
      "Epoch 4 - loss: 12.5521 - acc: 0.9800\n",
      "Epoch 5 - loss: 11.2150 - acc: 1.0000\n",
      "Epoch 6 - loss: 10.4222 - acc: 0.9800\n",
      "Epoch 7 - loss: 9.6837 - acc: 1.0000\n",
      "Epoch 8 - loss: 9.2941 - acc: 0.9800\n",
      "Epoch 9 - loss: 8.6624 - acc: 0.9800\n",
      "Epoch 10 - loss: 9.0925 - acc: 0.9700\n",
      "Epoch 11 - loss: 8.6751 - acc: 0.9900\n",
      "Epoch 12 - loss: 8.8580 - acc: 1.0000\n",
      "Epoch 13 - loss: 8.2770 - acc: 0.9500\n",
      "Epoch 14 - loss: 8.3462 - acc: 0.9700\n",
      "Epoch 15 - loss: 8.9685 - acc: 1.0000\n",
      "Epoch 16 - loss: 9.3635 - acc: 1.0000\n",
      "Epoch 17 - loss: 9.0657 - acc: 1.0000\n",
      "Epoch 18 - loss: 4.8505 - acc: 1.0000\n",
      "Epoch 19 - loss: 2.3011 - acc: 1.0000\n",
      "Epoch 20 - loss: 1.9291 - acc: 1.0000\n",
      "Epoch 21 - loss: 1.8616 - acc: 0.9900\n",
      "Epoch 22 - loss: 2.4705 - acc: 0.9900\n",
      "Epoch 23 - loss: 2.3905 - acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - loss: 2.4755 - acc: 1.0000\n",
      "Epoch 25 - loss: 2.5422 - acc: 0.9900\n",
      "Epoch 26 - loss: 2.7691 - acc: 0.9900\n",
      "Epoch 27 - loss: 3.2947 - acc: 1.0000\n",
      "Epoch 28 - loss: 3.3034 - acc: 0.9900\n",
      "Epoch 29 - loss: 3.0914 - acc: 1.0000\n",
      "Epoch 30 - loss: 3.7632 - acc: 0.9900\n",
      "Epoch 31 - loss: 3.5340 - acc: 1.0000\n",
      "Epoch 32 - loss: 3.0777 - acc: 1.0000\n",
      "Epoch 33 - loss: 3.4628 - acc: 0.9800\n",
      "Epoch 34 - loss: 3.2767 - acc: 1.0000\n",
      "Epoch 35 - loss: 3.4457 - acc: 1.0000\n",
      "Epoch 36 - loss: 2.8658 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.9566 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.7001 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.8195 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.8793 - acc: 1.0000\n",
      "Epoch 41 - loss: 0.8949 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.8219 - acc: 0.9900\n",
      "Epoch 43 - loss: 0.9945 - acc: 1.0000\n",
      "Epoch 44 - loss: 1.1017 - acc: 1.0000\n",
      "Epoch 45 - loss: 0.9543 - acc: 1.0000\n",
      "Epoch 46 - loss: 1.1166 - acc: 1.0000\n",
      "Epoch 47 - loss: 1.1764 - acc: 1.0000\n",
      "Epoch 48 - loss: 1.0234 - acc: 1.0000\n",
      "Epoch 49 - loss: 1.4125 - acc: 1.0000\n",
      "Epoch 0 - loss: 47.4188 - acc: 0.9500\n",
      "Epoch 1 - loss: 29.1834 - acc: 0.9700\n",
      "Epoch 2 - loss: 24.8054 - acc: 0.9800\n",
      "Epoch 3 - loss: 20.0766 - acc: 0.9800\n",
      "Epoch 4 - loss: 16.0343 - acc: 0.9900\n",
      "Epoch 5 - loss: 13.4662 - acc: 0.9900\n",
      "Epoch 6 - loss: 11.5349 - acc: 0.9900\n",
      "Epoch 7 - loss: 10.1898 - acc: 0.9800\n",
      "Epoch 8 - loss: 9.1672 - acc: 0.9900\n",
      "Epoch 9 - loss: 8.1069 - acc: 0.9900\n",
      "Epoch 10 - loss: 7.5160 - acc: 0.9900\n",
      "Epoch 11 - loss: 6.8686 - acc: 1.0000\n",
      "Epoch 12 - loss: 6.1003 - acc: 0.9900\n",
      "Epoch 13 - loss: 5.9134 - acc: 0.9900\n",
      "Epoch 14 - loss: 5.3907 - acc: 0.9900\n",
      "Epoch 15 - loss: 5.0936 - acc: 0.9900\n",
      "Epoch 16 - loss: 5.0226 - acc: 0.9700\n",
      "Epoch 17 - loss: 4.9354 - acc: 0.9900\n",
      "Epoch 18 - loss: 3.2856 - acc: 1.0000\n",
      "Epoch 19 - loss: 2.2937 - acc: 1.0000\n",
      "Epoch 20 - loss: 2.1148 - acc: 1.0000\n",
      "Epoch 21 - loss: 2.0475 - acc: 1.0000\n",
      "Epoch 22 - loss: 2.2131 - acc: 1.0000\n",
      "Epoch 23 - loss: 2.2109 - acc: 1.0000\n",
      "Epoch 24 - loss: 2.1645 - acc: 1.0000\n",
      "Epoch 25 - loss: 2.1365 - acc: 1.0000\n",
      "Epoch 26 - loss: 2.1989 - acc: 1.0000\n",
      "Epoch 27 - loss: 2.1277 - acc: 1.0000\n",
      "Epoch 28 - loss: 2.1669 - acc: 1.0000\n",
      "Epoch 29 - loss: 2.1882 - acc: 1.0000\n",
      "Epoch 30 - loss: 2.1800 - acc: 1.0000\n",
      "Epoch 31 - loss: 2.1526 - acc: 1.0000\n",
      "Epoch 32 - loss: 2.1150 - acc: 1.0000\n",
      "Epoch 33 - loss: 2.1905 - acc: 1.0000\n",
      "Epoch 34 - loss: 2.2339 - acc: 0.9700\n",
      "Epoch 35 - loss: 2.3289 - acc: 1.0000\n",
      "Epoch 36 - loss: 1.5754 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.9146 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.9062 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.8919 - acc: 1.0000\n",
      "Epoch 40 - loss: 1.0009 - acc: 1.0000\n",
      "Epoch 41 - loss: 1.0277 - acc: 1.0000\n",
      "Epoch 42 - loss: 1.0607 - acc: 0.9900\n",
      "Epoch 43 - loss: 1.1156 - acc: 1.0000\n",
      "Epoch 44 - loss: 1.1128 - acc: 0.9900\n",
      "Epoch 45 - loss: 1.0962 - acc: 0.9900\n",
      "Epoch 46 - loss: 0.9571 - acc: 1.0000\n",
      "Epoch 47 - loss: 0.9733 - acc: 0.9900\n",
      "Epoch 48 - loss: 1.0282 - acc: 0.9900\n",
      "Epoch 49 - loss: 1.1780 - acc: 1.0000\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n",
      "Epoch 0 - loss: nan - acc: 0.1000\n",
      "Epoch 1 - loss: nan - acc: 0.1000\n",
      "Epoch 2 - loss: nan - acc: 0.1000\n",
      "Epoch 3 - loss: nan - acc: 0.1000\n",
      "Epoch 4 - loss: nan - acc: 0.1000\n",
      "Epoch 5 - loss: nan - acc: 0.1000\n",
      "Epoch 6 - loss: nan - acc: 0.1000\n",
      "Epoch 7 - loss: nan - acc: 0.1000\n",
      "Epoch 8 - loss: nan - acc: 0.1000\n",
      "Epoch 9 - loss: nan - acc: 0.1000\n",
      "Epoch 10 - loss: nan - acc: 0.1000\n",
      "Epoch 11 - loss: nan - acc: 0.1000\n",
      "Epoch 12 - loss: nan - acc: 0.1000\n",
      "Epoch 13 - loss: nan - acc: 0.1000\n",
      "Epoch 14 - loss: nan - acc: 0.1000\n",
      "Epoch 15 - loss: nan - acc: 0.1000\n",
      "Epoch 16 - loss: nan - acc: 0.1000\n",
      "Epoch 17 - loss: nan - acc: 0.1000\n",
      "Epoch 18 - loss: nan - acc: 0.1000\n",
      "Epoch 19 - loss: nan - acc: 0.1000\n",
      "Epoch 20 - loss: nan - acc: 0.1000\n",
      "Epoch 21 - loss: nan - acc: 0.1000\n",
      "Epoch 22 - loss: nan - acc: 0.1000\n",
      "Epoch 23 - loss: nan - acc: 0.1000\n",
      "Epoch 24 - loss: nan - acc: 0.1000\n",
      "Epoch 25 - loss: nan - acc: 0.1000\n",
      "Epoch 26 - loss: nan - acc: 0.1000\n",
      "Epoch 27 - loss: nan - acc: 0.1000\n",
      "Epoch 28 - loss: nan - acc: 0.1000\n",
      "Epoch 29 - loss: nan - acc: 0.1000\n",
      "Epoch 30 - loss: nan - acc: 0.1000\n",
      "Epoch 31 - loss: nan - acc: 0.1000\n",
      "Epoch 32 - loss: nan - acc: 0.1000\n",
      "Epoch 33 - loss: nan - acc: 0.1000\n",
      "Epoch 34 - loss: nan - acc: 0.1000\n",
      "Epoch 35 - loss: nan - acc: 0.1000\n",
      "Epoch 36 - loss: nan - acc: 0.1000\n",
      "Epoch 37 - loss: nan - acc: 0.1000\n",
      "Epoch 38 - loss: nan - acc: 0.1000\n",
      "Epoch 39 - loss: nan - acc: 0.1000\n",
      "Epoch 40 - loss: nan - acc: 0.1000\n",
      "Epoch 41 - loss: nan - acc: 0.1000\n",
      "Epoch 42 - loss: nan - acc: 0.1000\n",
      "Epoch 43 - loss: nan - acc: 0.1000\n",
      "Epoch 44 - loss: nan - acc: 0.1000\n",
      "Epoch 45 - loss: nan - acc: 0.1000\n",
      "Epoch 46 - loss: nan - acc: 0.1000\n",
      "Epoch 47 - loss: nan - acc: 0.1000\n",
      "Epoch 48 - loss: nan - acc: 0.1000\n",
      "Epoch 49 - loss: nan - acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import sgld\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "lambda_ = 1.\n",
    "#lr = 3e-6\n",
    "\n",
    "# learning_rates = [1e-6, 1e-7, 1e-8, 1e-9]\n",
    "learning_rates = [6e-7, 5e-6, 4e-6, 3e-6, 2e-6, 1e-6, 9e-5, 8e-5, 7e-5, 6e-5, 5e-5]\n",
    "\n",
    "\n",
    "t = 1\n",
    "n = 0\n",
    "\n",
    "error_results = []\n",
    "# losses_sgld = []\n",
    "# acc_sgld = []\n",
    "for lr in learning_rates:\n",
    "    network = Model()\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    optim = sgld.optim.sgld(network, lr, lambda_, batch_size, dataset_size)\n",
    "    evaluate = evaluation(val_loader, criterion)\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0\n",
    "        for x, y in iter(train_loader):\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            network.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_loss += loss * batch_size / dataset_size\n",
    "            prediction = output.data.max(1)[1]\n",
    "            accuracy = torch.sum(prediction.eq(y)).float()/batch_size\n",
    "\n",
    "\n",
    "            if (t >= 300) & (t % 100 == 0):\n",
    "                loss, acc = evaluate.acc(network)\n",
    "\n",
    "\n",
    "            t += 1.\n",
    "\n",
    "    #     losses_sgld.append(loss)\n",
    "    #     acc_sgld.append(acc)\n",
    "\n",
    "        print(\"Epoch {:d} - loss: {:.4f} - acc: {:.4f}\".format(epoch, running_loss, accuracy))\n",
    "\n",
    "    error_sgld = 100. - acc\n",
    "    error_results.append(error_sgld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.0200),\n",
       " tensor(89.2600),\n",
       " tensor(89.2600),\n",
       " tensor(1.5400),\n",
       " tensor(2.0400),\n",
       " tensor(1.9400),\n",
       " tensor(89.2600),\n",
       " tensor(89.2600),\n",
       " tensor(89.2600),\n",
       " tensor(89.2600),\n",
       " tensor(89.2600)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_results\n",
    "# 3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2, 2.4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(range(len(acc_sgld)),acc_sgld)\n",
    "plt.ylim(1.2,2.4)\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Test Set Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f52f53ab9d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import psgld\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "lambda_ = 1.\n",
    "#lr = 3e-6\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "alpha = 0.99\n",
    "\n",
    "\n",
    "t = 1\n",
    "n = 0\n",
    "\n",
    "error_psgld = []\n",
    "# losses_sgld = []\n",
    "# acc_sgld = []\n",
    "for lr in learning_rates:\n",
    "    network = Model()\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    optim = psgld.optim.psgld(network, lr, alpha, lambda_, batch_size, dataset_size)\n",
    "    evaluate = evaluation(val_loader, criterion)\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0\n",
    "        for x, y in iter(train_loader):\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            network.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_loss += loss * batch_size / dataset_size\n",
    "            prediction = output.data.max(1)[1]\n",
    "            accuracy = torch.sum(prediction.eq(y)).float()/batch_size\n",
    "\n",
    "\n",
    "            if (t >= 300) & (t % 100 == 0):\n",
    "                loss, acc = evaluate.acc(network)\n",
    "\n",
    "\n",
    "            t += 1.\n",
    "\n",
    "    #     losses_psgld.append(loss)\n",
    "    #     acc_psgld.append(acc)\n",
    "\n",
    "        print(\"Epoch {:d} - loss: {:.4f} - acc: {:.4f}\".format(epoch, running_loss, accuracy))\n",
    "\n",
    "    error = 100. - acc\n",
    "    error_psgld.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psgld\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "lambda_ = 1.\n",
    "#lr = 3e-6\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "alpha = 0.99\n",
    "\n",
    "\n",
    "t = 1\n",
    "n = 0\n",
    "\n",
    "error_psgld = []\n",
    "# losses_sgld = []\n",
    "# acc_sgld = []\n",
    "for lr in learning_rates:\n",
    "    network = Model()\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    optim = psgld.optim.psgld(network, lr, alpha, lambda_, batch_size, dataset_size)\n",
    "    evaluate = evaluation(val_loader, criterion)\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0\n",
    "        for x, y in iter(train_loader):\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            network.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_loss += loss * batch_size / dataset_size\n",
    "            prediction = output.data.max(1)[1]\n",
    "            accuracy = torch.sum(prediction.eq(y)).float()/batch_size\n",
    "\n",
    "\n",
    "            if (t >= 300) & (t % 100 == 0):\n",
    "                loss, acc = evaluate.acc(network)\n",
    "\n",
    "\n",
    "            t += 1.\n",
    "\n",
    "    #     losses_psgld.append(loss)\n",
    "    #     acc_psgld.append(acc)\n",
    "\n",
    "        print(\"Epoch {:d} - loss: {:.4f} - acc: {:.4f}\".format(epoch, running_loss, accuracy))\n",
    "\n",
    "    error = 100. - acc\n",
    "    error_psgld.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/opt/anaconda/envs/Python3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 35.5843 - acc: 0.9300\n",
      "Epoch 1 - loss: 22.3452 - acc: 0.9500\n",
      "Epoch 2 - loss: 16.6136 - acc: 0.9700\n",
      "Epoch 3 - loss: 12.3814 - acc: 0.9700\n",
      "Epoch 4 - loss: 9.5400 - acc: 0.9700\n",
      "Epoch 5 - loss: 7.5571 - acc: 0.9700\n",
      "Epoch 6 - loss: 5.9602 - acc: 0.9800\n",
      "Epoch 7 - loss: 4.6991 - acc: 0.9900\n",
      "Epoch 8 - loss: 3.6381 - acc: 0.9900\n",
      "Epoch 9 - loss: 2.8270 - acc: 1.0000\n",
      "Epoch 10 - loss: 2.1282 - acc: 1.0000\n",
      "Epoch 11 - loss: 1.5317 - acc: 1.0000\n",
      "Epoch 12 - loss: 1.1456 - acc: 1.0000\n",
      "Epoch 13 - loss: 0.8095 - acc: 1.0000\n",
      "Epoch 14 - loss: 0.5908 - acc: 1.0000\n",
      "Epoch 15 - loss: 0.5063 - acc: 1.0000\n",
      "Epoch 16 - loss: 0.3699 - acc: 1.0000\n",
      "Epoch 17 - loss: 0.3127 - acc: 1.0000\n",
      "Epoch 18 - loss: 0.2335 - acc: 1.0000\n",
      "Epoch 19 - loss: 0.1929 - acc: 1.0000\n",
      "Epoch 20 - loss: 0.1626 - acc: 1.0000\n",
      "Epoch 21 - loss: 0.1416 - acc: 1.0000\n",
      "Epoch 22 - loss: 0.1279 - acc: 1.0000\n",
      "Epoch 25 - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 26 - loss: 0.0961 - acc: 1.0000\n",
      "Epoch 27 - loss: 0.0906 - acc: 1.0000\n",
      "Epoch 28 - loss: 0.0861 - acc: 1.0000\n",
      "Epoch 29 - loss: 0.0822 - acc: 1.0000\n",
      "Epoch 30 - loss: 0.0788 - acc: 1.0000\n",
      "Epoch 31 - loss: 0.0756 - acc: 1.0000\n",
      "Epoch 32 - loss: 0.0728 - acc: 1.0000\n",
      "Epoch 33 - loss: 0.0703 - acc: 1.0000\n",
      "Epoch 34 - loss: 0.0681 - acc: 1.0000\n",
      "Epoch 35 - loss: 0.0660 - acc: 1.0000\n",
      "Epoch 36 - loss: 0.0641 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.0624 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.0594 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.0581 - acc: 1.0000\n",
      "Epoch 41 - loss: 0.0569 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 43 - loss: 0.0547 - acc: 1.0000\n",
      "Epoch 44 - loss: 0.0538 - acc: 1.0000\n",
      "Epoch 45 - loss: 0.0528 - acc: 1.0000\n",
      "Epoch 46 - loss: 0.0519 - acc: 1.0000\n",
      "Epoch 47 - loss: 0.0511 - acc: 1.0000\n",
      "Epoch 48 - loss: 0.0503 - acc: 1.0000\n",
      "Epoch 49 - loss: 0.0496 - acc: 1.0000\n",
      "Epoch 0 - loss: 35.6100 - acc: 0.9500\n",
      "Epoch 1 - loss: 22.6772 - acc: 0.9600\n",
      "Epoch 2 - loss: 17.3385 - acc: 0.9700\n",
      "Epoch 3 - loss: 13.1796 - acc: 0.9700\n",
      "Epoch 4 - loss: 10.2390 - acc: 0.9700\n",
      "Epoch 5 - loss: 8.1329 - acc: 0.9800\n",
      "Epoch 6 - loss: 6.4120 - acc: 0.9800\n",
      "Epoch 7 - loss: 5.0904 - acc: 0.9800\n",
      "Epoch 8 - loss: 3.9815 - acc: 0.9900\n",
      "Epoch 9 - loss: 3.0833 - acc: 1.0000\n",
      "Epoch 10 - loss: 2.3806 - acc: 1.0000\n",
      "Epoch 11 - loss: 1.7183 - acc: 1.0000\n",
      "Epoch 12 - loss: 1.2841 - acc: 0.9900\n",
      "Epoch 13 - loss: 0.9031 - acc: 1.0000\n",
      "Epoch 14 - loss: 0.5900 - acc: 1.0000\n",
      "Epoch 15 - loss: 0.4283 - acc: 1.0000\n",
      "Epoch 16 - loss: 0.3194 - acc: 1.0000\n",
      "Epoch 17 - loss: 0.2540 - acc: 1.0000\n",
      "Epoch 18 - loss: 0.2087 - acc: 1.0000\n",
      "Epoch 19 - loss: 0.1811 - acc: 1.0000\n",
      "Epoch 20 - loss: 0.1582 - acc: 1.0000\n",
      "Epoch 21 - loss: 0.1406 - acc: 1.0000\n",
      "Epoch 22 - loss: 0.1274 - acc: 1.0000\n",
      "Epoch 23 - loss: 0.1162 - acc: 1.0000\n",
      "Epoch 24 - loss: 0.1080 - acc: 1.0000\n",
      "Epoch 25 - loss: 0.1006 - acc: 1.0000\n",
      "Epoch 26 - loss: 0.0943 - acc: 1.0000\n",
      "Epoch 27 - loss: 0.0891 - acc: 1.0000\n",
      "Epoch 28 - loss: 0.0845 - acc: 1.0000\n",
      "Epoch 29 - loss: 0.0805 - acc: 1.0000\n",
      "Epoch 30 - loss: 0.0771 - acc: 1.0000\n",
      "Epoch 31 - loss: 0.0740 - acc: 1.0000\n",
      "Epoch 32 - loss: 0.0711 - acc: 1.0000\n",
      "Epoch 33 - loss: 0.0686 - acc: 1.0000\n",
      "Epoch 34 - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 35 - loss: 0.0643 - acc: 1.0000\n",
      "Epoch 36 - loss: 0.0626 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.0592 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.0578 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.0565 - acc: 1.0000\n",
      "Epoch 41 - loss: 0.0552 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.0542 - acc: 1.0000\n",
      "Epoch 43 - loss: 0.0531 - acc: 1.0000\n",
      "Epoch 44 - loss: 0.0521 - acc: 1.0000\n",
      "Epoch 45 - loss: 0.0512 - acc: 1.0000\n",
      "Epoch 46 - loss: 0.0504 - acc: 1.0000\n",
      "Epoch 47 - loss: 0.0495 - acc: 1.0000\n",
      "Epoch 48 - loss: 0.0488 - acc: 1.0000\n",
      "Epoch 49 - loss: 0.0481 - acc: 1.0000\n",
      "Epoch 0 - loss: 36.0725 - acc: 0.9400\n",
      "Epoch 2 - loss: 17.4835 - acc: 0.9500\n",
      "Epoch 3 - loss: 13.4520 - acc: 0.9700\n",
      "Epoch 4 - loss: 10.5722 - acc: 0.9700\n",
      "Epoch 5 - loss: 8.4468 - acc: 0.9700\n",
      "Epoch 6 - loss: 6.8080 - acc: 0.9800\n",
      "Epoch 7 - loss: 5.4208 - acc: 0.9900\n",
      "Epoch 8 - loss: 4.3507 - acc: 0.9800\n",
      "Epoch 9 - loss: 3.4244 - acc: 0.9900\n",
      "Epoch 10 - loss: 2.6393 - acc: 0.9900\n",
      "Epoch 11 - loss: 2.0692 - acc: 1.0000\n",
      "Epoch 12 - loss: 1.6178 - acc: 0.9900\n",
      "Epoch 13 - loss: 1.1863 - acc: 1.0000\n",
      "Epoch 14 - loss: 0.8156 - acc: 1.0000\n",
      "Epoch 15 - loss: 0.5892 - acc: 1.0000\n",
      "Epoch 16 - loss: 0.3885 - acc: 1.0000\n",
      "Epoch 17 - loss: 0.2918 - acc: 1.0000\n",
      "Epoch 18 - loss: 0.2337 - acc: 1.0000\n",
      "Epoch 19 - loss: 0.2023 - acc: 1.0000\n",
      "Epoch 20 - loss: 0.1717 - acc: 1.0000\n",
      "Epoch 21 - loss: 0.1530 - acc: 1.0000\n",
      "Epoch 22 - loss: 0.1377 - acc: 1.0000\n",
      "Epoch 23 - loss: 0.1270 - acc: 1.0000\n",
      "Epoch 24 - loss: 0.1168 - acc: 1.0000\n",
      "Epoch 26 - loss: 0.1026 - acc: 1.0000\n",
      "Epoch 27 - loss: 0.0966 - acc: 1.0000\n",
      "Epoch 28 - loss: 0.0917 - acc: 1.0000\n",
      "Epoch 29 - loss: 0.0871 - acc: 1.0000\n",
      "Epoch 30 - loss: 0.0834 - acc: 1.0000\n",
      "Epoch 31 - loss: 0.0800 - acc: 1.0000\n",
      "Epoch 32 - loss: 0.0769 - acc: 1.0000\n",
      "Epoch 33 - loss: 0.0741 - acc: 1.0000\n",
      "Epoch 34 - loss: 0.0718 - acc: 1.0000\n",
      "Epoch 35 - loss: 0.0694 - acc: 1.0000\n",
      "Epoch 36 - loss: 0.0673 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.0653 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.0636 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.0621 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.0606 - acc: 1.0000\n",
      "Epoch 41 - loss: 0.0592 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.0579 - acc: 1.0000\n",
      "Epoch 43 - loss: 0.0568 - acc: 1.0000\n",
      "Epoch 44 - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 45 - loss: 0.0547 - acc: 1.0000\n",
      "Epoch 46 - loss: 0.0537 - acc: 1.0000\n",
      "Epoch 47 - loss: 0.0528 - acc: 1.0000\n",
      "Epoch 48 - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 49 - loss: 0.0511 - acc: 1.0000\n",
      "Epoch 0 - loss: 40.3982 - acc: 0.9300\n",
      "Epoch 1 - loss: 24.5596 - acc: 0.9500\n",
      "Epoch 2 - loss: 20.7290 - acc: 0.9700\n",
      "Epoch 3 - loss: 17.3489 - acc: 0.9700\n",
      "Epoch 4 - loss: 14.4643 - acc: 0.9600\n",
      "Epoch 5 - loss: 12.0676 - acc: 0.9600\n",
      "Epoch 6 - loss: 10.0986 - acc: 0.9700\n",
      "Epoch 7 - loss: 8.4535 - acc: 0.9600\n",
      "Epoch 8 - loss: 7.1792 - acc: 0.9700\n",
      "Epoch 9 - loss: 6.0296 - acc: 0.9600\n",
      "Epoch 10 - loss: 5.0978 - acc: 0.9700\n",
      "Epoch 11 - loss: 4.1864 - acc: 0.9600\n",
      "Epoch 12 - loss: 3.4699 - acc: 0.9800\n",
      "Epoch 13 - loss: 2.8884 - acc: 0.9900\n",
      "Epoch 14 - loss: 2.4033 - acc: 0.9900\n",
      "Epoch 15 - loss: 1.9760 - acc: 0.9900\n",
      "Epoch 16 - loss: 1.6674 - acc: 1.0000\n",
      "Epoch 17 - loss: 1.3335 - acc: 1.0000\n",
      "Epoch 18 - loss: 1.1193 - acc: 1.0000\n",
      "Epoch 19 - loss: 0.8536 - acc: 1.0000\n",
      "Epoch 20 - loss: 0.6518 - acc: 1.0000\n",
      "Epoch 21 - loss: 0.4860 - acc: 1.0000\n",
      "Epoch 22 - loss: 0.3945 - acc: 1.0000\n",
      "Epoch 23 - loss: 0.3259 - acc: 1.0000\n",
      "Epoch 24 - loss: 0.2823 - acc: 1.0000\n",
      "Epoch 25 - loss: 0.2455 - acc: 1.0000\n",
      "Epoch 26 - loss: 0.2241 - acc: 1.0000\n",
      "Epoch 27 - loss: 0.2000 - acc: 1.0000\n",
      "Epoch 28 - loss: 0.1828 - acc: 1.0000\n",
      "Epoch 29 - loss: 0.1679 - acc: 1.0000\n",
      "Epoch 30 - loss: 0.1584 - acc: 1.0000\n",
      "Epoch 31 - loss: 0.1470 - acc: 1.0000\n",
      "Epoch 32 - loss: 0.1377 - acc: 1.0000\n",
      "Epoch 33 - loss: 0.1296 - acc: 1.0000\n",
      "Epoch 34 - loss: 0.1229 - acc: 1.0000\n",
      "Epoch 35 - loss: 0.1169 - acc: 1.0000\n",
      "Epoch 36 - loss: 0.1114 - acc: 1.0000\n",
      "Epoch 37 - loss: 0.1063 - acc: 1.0000\n",
      "Epoch 38 - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 39 - loss: 0.0982 - acc: 1.0000\n",
      "Epoch 40 - loss: 0.0945 - acc: 1.0000\n",
      "Epoch 41 - loss: 0.0914 - acc: 1.0000\n",
      "Epoch 42 - loss: 0.0883 - acc: 1.0000\n",
      "Epoch 43 - loss: 0.0857 - acc: 1.0000\n",
      "Epoch 44 - loss: 0.0830 - acc: 1.0000\n",
      "Epoch 45 - loss: 0.0806 - acc: 1.0000\n",
      "Epoch 46 - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 47 - loss: 0.0763 - acc: 1.0000\n",
      "Epoch 48 - loss: 0.0743 - acc: 1.0000\n",
      "Epoch 49 - loss: 0.0721 - acc: 1.0000\n",
      "Epoch 0 - loss: 76.1338 - acc: 0.8800\n",
      "Epoch 1 - loss: 33.1959 - acc: 0.9300\n",
      "Epoch 2 - loss: 29.0184 - acc: 0.9400\n",
      "Epoch 3 - loss: 26.9100 - acc: 0.9300\n",
      "Epoch 4 - loss: 25.4297 - acc: 0.9300\n",
      "Epoch 5 - loss: 24.2069 - acc: 0.9300\n",
      "Epoch 6 - loss: 23.1360 - acc: 0.9500\n",
      "Epoch 7 - loss: 22.1486 - acc: 0.9500\n",
      "Epoch 8 - loss: 21.2104 - acc: 0.9500\n",
      "Epoch 9 - loss: 20.3032 - acc: 0.9600\n",
      "Epoch 10 - loss: 19.4050 - acc: 0.9600\n",
      "Epoch 11 - loss: 18.5218 - acc: 0.9600\n",
      "Epoch 12 - loss: 17.6609 - acc: 0.9500\n",
      "Epoch 13 - loss: 16.7715 - acc: 0.9600\n",
      "Epoch 14 - loss: 15.8986 - acc: 0.9600\n",
      "Epoch 15 - loss: 15.0558 - acc: 0.9600\n",
      "Epoch 16 - loss: 14.2370 - acc: 0.9600\n",
      "Epoch 17 - loss: 13.4472 - acc: 0.9600\n",
      "Epoch 18 - loss: 12.6960 - acc: 0.9700\n",
      "Epoch 19 - loss: 11.9847 - acc: 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - loss: 11.2715 - acc: 0.9600\n",
      "Epoch 21 - loss: 10.6419 - acc: 0.9800\n",
      "Epoch 22 - loss: 10.0308 - acc: 0.9900\n",
      "Epoch 23 - loss: 9.4492 - acc: 0.9700\n",
      "Epoch 24 - loss: 8.9077 - acc: 0.9900\n",
      "Epoch 25 - loss: 8.4211 - acc: 0.9900\n",
      "Epoch 26 - loss: 7.9269 - acc: 0.9900\n",
      "Epoch 27 - loss: 7.4620 - acc: 0.9900\n",
      "Epoch 28 - loss: 7.0598 - acc: 0.9900\n",
      "Epoch 30 - loss: 6.2318 - acc: 0.9900\n",
      "Epoch 31 - loss: 5.8952 - acc: 0.9900\n",
      "Epoch 32 - loss: 5.5556 - acc: 0.9900\n",
      "Epoch 33 - loss: 5.2712 - acc: 0.9900\n",
      "Epoch 34 - loss: 4.9609 - acc: 0.9900\n",
      "Epoch 35 - loss: 4.5778 - acc: 0.9900\n",
      "Epoch 36 - loss: 4.3337 - acc: 0.9900\n",
      "Epoch 37 - loss: 4.0578 - acc: 0.9900\n",
      "Epoch 38 - loss: 3.7983 - acc: 0.9900\n",
      "Epoch 39 - loss: 3.6047 - acc: 0.9900\n",
      "Epoch 40 - loss: 3.3435 - acc: 0.9900\n",
      "Epoch 41 - loss: 3.1705 - acc: 0.9900\n",
      "Epoch 42 - loss: 2.9431 - acc: 0.9900\n",
      "Epoch 43 - loss: 2.7829 - acc: 0.9900\n",
      "Epoch 44 - loss: 2.6189 - acc: 0.9900\n",
      "Epoch 45 - loss: 2.4121 - acc: 1.0000\n",
      "Epoch 46 - loss: 2.2940 - acc: 1.0000\n",
      "Epoch 47 - loss: 2.1450 - acc: 1.0000\n",
      "Epoch 48 - loss: 2.0386 - acc: 0.9900\n",
      "Epoch 49 - loss: 1.9287 - acc: 0.9900\n",
      "Epoch 0 - loss: 196.8971 - acc: 0.7500\n",
      "Epoch 1 - loss: 124.2864 - acc: 0.8200\n",
      "Epoch 2 - loss: 80.3312 - acc: 0.8600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4d5e01c8f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_curvature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/sgfs/SGFS/ksgfs/optim.py\u001b[0m in \u001b[0;36mupdate_curvature\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minversion_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minversion_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_curvature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvert_curvature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/sgfs/SGFS/ksgfs/optim.py\u001b[0m in \u001b[0;36minvert_curvature\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvert_curvature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invert_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreactivation_fishers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreactivation_fisher_inverses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invert_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_covariances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_covariance_inverses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_invert_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/sgfs/SGFS/ksgfs/optim.py\u001b[0m in \u001b[0;36m_invert_fn\u001b[0;34m(self, d, inv_dict)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_invert_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             w, v, info = evr(a1, uplo=uplo, jobz=_job, range=\"A\", il=1,\n\u001b[0;32m--> 432\u001b[0;31m                              iu=a1.shape[0], overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ksgfs\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "\n",
    "#lr = 3e-6\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "eta = 1.\n",
    "v = 0\n",
    "lambda_ = 1.\n",
    "l2 = 1e-3\n",
    "\n",
    "\n",
    "t = 1\n",
    "n = 0\n",
    "\n",
    "error_psgld = []\n",
    "# losses_ksgfs = []\n",
    "# acc_ksgfs = []\n",
    "for lr in learning_rates:\n",
    "    network = Model()\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    optim = ksgfs.optim.KSGFS(network, criterion, batch_size, dataset_size, eta=eta, v=v, lambda_=lambda_, epsilon=lr, l2=l2, invert_every=1)\n",
    "    evaluate = evaluation(val_loader, criterion)\n",
    "    for epoch in range(50):\n",
    "        running_loss = 0\n",
    "        for x, y in iter(train_loader):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            \n",
    "            optim.update_curvature(x)\n",
    "\n",
    "            network.zero_grad()\n",
    "            output = network(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            running_loss += loss * batch_size / dataset_size\n",
    "            prediction = output.data.max(1)[1]\n",
    "            accuracy = torch.sum(prediction.eq(y)).float()/batch_size\n",
    "\n",
    "\n",
    "            if (t >= 300) & (t % 100 == 0):\n",
    "                loss, acc = evaluate.acc(network)\n",
    "\n",
    "            t += 1.\n",
    "\n",
    "    #     losses_psgld.append(loss)\n",
    "    #     acc_psgld.append(acc)\n",
    "\n",
    "        print(\"Epoch {:d} - loss: {:.4f} - acc: {:.4f}\".format(epoch, running_loss, accuracy))\n",
    "\n",
    "    error = 100. - acc\n",
    "    error_psgld.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.8800), tensor(2.8000), tensor(2.8800), tensor(3.), tensor(4.2200)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_psgld\n",
    "# 1e-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
